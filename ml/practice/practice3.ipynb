{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cdd0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e580e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f3873df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    median_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    explained_variance_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58cd618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def perform_random_search(X_train, y_train):\n",
    "    print(\"\\n[STEP] Performing RandomizedSearchCV...\")\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    param_dist = {\n",
    "        \"n_estimators\": randint(20, 40),  # reduced upper bound\n",
    "        \"max_depth\": [10, 15, 20],\n",
    "        \"min_samples_split\": randint(2, 6),\n",
    "        \"min_samples_leaf\": randint(1, 4),\n",
    "        \"max_features\": [0.4, 0.5, 0.6],  # avoid uniform, use discrete\n",
    "        \"max_leaf_nodes\": [100, 200, None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,  # lower iterations to avoid long wait\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(f\"[INFO] Best Parameters: {random_search.best_params_}\")\n",
    "    return random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d62c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, X_test, y_test, model_path, start_time):\n",
    "    print(\"\\n[STEP] Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Absolute Error (MAE)         : {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE)          : {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE)    : {rmse:.4f}\")\n",
    "    print(f\"R² Score                          : {r2:.4f}\")\n",
    "    print(f\"Median Absolute Error (MedAE)     : {medae:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error    : {mape:.4f}\")\n",
    "    print(f\"Explained Variance Score (EVS)    : {evs:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n[STEP] Saving model to disk...\")\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    joblib.dump(model, model_path, compress=3)\n",
    "\n",
    "    # Estimate model size\n",
    "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"[INFO] Model saved to: {model_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"[INFO] Total training + saving time: {round(total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84d1dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_temperature_data(csv_path):\n",
    "    print(f\"\\n[INFO] Reading CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Original DataFrame shape:\", df.shape)\n",
    "    \n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"[DEBUG] Warning: Null values found. Proceeding anyway.\")\n",
    "\n",
    "    # Rename and convert datetime\n",
    "    df.rename(columns={'temperature_2m (°C)': 'temperature', 'time': 'datetime'}, inplace=True)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    print(\"[INFO] Converted datetime and sorted. Shape:\", df.shape)\n",
    "\n",
    "    # Lag features\n",
    "    for i in range(1, 25):\n",
    "        df[f'temp_t-{i}'] = df['temperature'].shift(i)\n",
    "    \n",
    "    df['target'] = df['temperature'].shift(-1)\n",
    "\n",
    "    print(\"[DEBUG] After adding lag + target. Shape:\", df.shape)\n",
    "    print(\"[DEBUG] Preview of columns:\", df.columns.tolist())\n",
    "\n",
    "    # Drop NA from lag creation\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    print(\"[INFO] Dropped NaNs. Final usable shape:\", df.shape)\n",
    "\n",
    "    # Define X and y\n",
    "    feature_cols = [f'temp_t-{i}' for i in range(1, 25)]\n",
    "    X = df[feature_cols]\n",
    "    y = df['target']\n",
    "\n",
    "    print(\"[DEBUG] Features shape:\", X.shape)\n",
    "    print(\"[DEBUG] Target shape:\", y.shape)\n",
    "    print(\"[DEBUG] Sample features:\\n\", X.head(2))\n",
    "    print(\"[DEBUG] Sample target:\\n\", y.head(2))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6248609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model Training\n",
    "def train_and_save_model(csv_path, model_path=\"ml/models/temp_next1hr_model.joblib\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\\n[STEP] Loading and preprocessing data...\")\n",
    "    X, y = preprocess_temperature_data(csv_path)\n",
    "\n",
    "    print(\"\\n[STEP] Splitting dataset into train/test...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"[INFO] X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    print(\"\\n[STEP] Training RandomForest model using RandomizedSearchCV...\")\n",
    "    model = perform_random_search(X_train, y_train)\n",
    "\n",
    "    \n",
    "    evaluation(model, X_test, y_test, model_path, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43f5e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP] Loading and preprocessing data...\n",
      "\n",
      "[INFO] Reading CSV: /home/aditya/flask/ml/dataset/open-meteo-18.62N74.00E561m.csv\n",
      "Original DataFrame shape: (131184, 10)\n",
      "[INFO] Converted datetime and sorted. Shape: (131184, 10)\n",
      "[DEBUG] After adding lag + target. Shape: (131184, 35)\n",
      "[DEBUG] Preview of columns: ['location_id', 'datetime', 'temperature', 'relative_humidity_2m (%)', 'dew_point_2m (°C)', 'rain (mm)', 'pressure_msl (hPa)', 'surface_pressure (hPa)', 'cloud_cover (%)', 'wind_speed_10m (km/h)', 'temp_t-1', 'temp_t-2', 'temp_t-3', 'temp_t-4', 'temp_t-5', 'temp_t-6', 'temp_t-7', 'temp_t-8', 'temp_t-9', 'temp_t-10', 'temp_t-11', 'temp_t-12', 'temp_t-13', 'temp_t-14', 'temp_t-15', 'temp_t-16', 'temp_t-17', 'temp_t-18', 'temp_t-19', 'temp_t-20', 'temp_t-21', 'temp_t-22', 'temp_t-23', 'temp_t-24', 'target']\n",
      "[INFO] Dropped NaNs. Final usable shape: (131159, 35)\n",
      "[DEBUG] Features shape: (131159, 24)\n",
      "[DEBUG] Target shape: (131159,)\n",
      "[DEBUG] Sample features:\n",
      "    temp_t-1  temp_t-2  temp_t-3  temp_t-4  temp_t-5  temp_t-6  temp_t-7  \\\n",
      "0      19.8      17.4      15.9      20.8      16.0      16.8      13.2   \n",
      "1      24.6      19.8      17.4      15.9      20.8      16.0      16.8   \n",
      "\n",
      "   temp_t-8  temp_t-9  temp_t-10  ...  temp_t-15  temp_t-16  temp_t-17  \\\n",
      "0      14.2      12.2       18.0  ...       18.1       13.1       17.0   \n",
      "1      13.2      14.2       12.2  ...       14.1       18.1       13.1   \n",
      "\n",
      "   temp_t-18  temp_t-19  temp_t-20  temp_t-21  temp_t-22  temp_t-23  temp_t-24  \n",
      "0       13.4       12.4       14.5       18.5       13.6       17.4       13.7  \n",
      "1       17.0       13.4       12.4       14.5       18.5       13.6       17.4  \n",
      "\n",
      "[2 rows x 24 columns]\n",
      "[DEBUG] Sample target:\n",
      " 0    19.7\n",
      "1    22.4\n",
      "Name: target, dtype: float64\n",
      "\n",
      "[STEP] Splitting dataset into train/test...\n",
      "[INFO] X_train: (104927, 24), X_test: (26232, 24)\n",
      "\n",
      "[STEP] Training RandomForest model using RandomizedSearchCV...\n",
      "\n",
      "[STEP] Performing RandomizedSearchCV...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=5, n_estimators=31; total time=  13.2s\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=5, n_estimators=31; total time=  13.6s\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=5, n_estimators=31; total time=  14.1s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=1, min_samples_split=2, n_estimators=31; total time=  14.7s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=1, min_samples_split=2, n_estimators=31; total time=  14.7s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=5, n_estimators=26; total time=  15.4s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=1, min_samples_split=2, n_estimators=31; total time=  15.4s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=5, n_estimators=26; total time=  16.7s\n",
      "[CV] END max_depth=15, max_features=0.6, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=4, n_estimators=23; total time=  16.8s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=5, n_estimators=26; total time=  17.1s\n",
      "[CV] END max_depth=15, max_features=0.6, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=4, n_estimators=23; total time=  17.7s\n",
      "[CV] END max_depth=15, max_features=0.6, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=4, n_estimators=23; total time=  17.8s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=1, min_samples_split=2, n_estimators=29; total time=  11.1s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=1, min_samples_split=2, n_estimators=29; total time=  12.6s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=1, min_samples_split=2, n_estimators=29; total time=  13.5s\n",
      "[CV] END max_depth=10, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=5, n_estimators=33; total time=  13.7s\n",
      "[CV] END max_depth=10, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=5, n_estimators=33; total time=  13.0s\n",
      "[CV] END max_depth=10, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=2, min_samples_split=5, n_estimators=33; total time=  13.0s\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=3, min_samples_split=3, n_estimators=38; total time=  15.8s\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=3, min_samples_split=3, n_estimators=38; total time=  16.2s\n",
      "[CV] END max_depth=20, max_features=0.5, max_leaf_nodes=200, min_samples_leaf=3, min_samples_split=3, n_estimators=38; total time=  16.0s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=4, n_estimators=26; total time=  15.1s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=2, n_estimators=21; total time=   6.6s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=2, n_estimators=21; total time=   7.4s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=4, n_estimators=26; total time=  17.2s\n",
      "[CV] END max_depth=20, max_features=0.4, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=4, n_estimators=26; total time=  16.4s\n",
      "[CV] END max_depth=15, max_features=0.5, max_leaf_nodes=100, min_samples_leaf=2, min_samples_split=2, n_estimators=21; total time=   6.0s\n",
      "[CV] END max_depth=20, max_features=0.6, max_leaf_nodes=100, min_samples_leaf=3, min_samples_split=4, n_estimators=33; total time=   4.6s\n",
      "[CV] END max_depth=20, max_features=0.6, max_leaf_nodes=100, min_samples_leaf=3, min_samples_split=4, n_estimators=33; total time=   4.7s\n",
      "[CV] END max_depth=20, max_features=0.6, max_leaf_nodes=100, min_samples_leaf=3, min_samples_split=4, n_estimators=33; total time=   4.8s\n",
      "[INFO] Best Parameters: {'max_depth': 20, 'max_features': 0.4, 'max_leaf_nodes': None, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 26}\n",
      "\n",
      "[STEP] Evaluating model...\n",
      "Mean Absolute Error (MAE)         : 1.2779\n",
      "Mean Squared Error (MSE)          : 2.8402\n",
      "Root Mean Squared Error (RMSE)    : 1.6853\n",
      "R² Score                          : 0.8947\n",
      "Median Absolute Error (MedAE)     : 0.9874\n",
      "Mean Absolute Percentage Error    : 0.0535\n",
      "Explained Variance Score (EVS)    : 0.8947\n",
      "\n",
      "[STEP] Saving model to disk...\n",
      "[INFO] Model saved to: ml/models/temp_next1hr_model.joblib (11.74 MB)\n",
      "[INFO] Total training + saving time: 39.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# ----------- Step 3: Run ------------\n",
    "# Replace with your actual file path\n",
    "train_and_save_model(\"/home/aditya/flask/ml/dataset/open-meteo-18.62N74.00E561m.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
